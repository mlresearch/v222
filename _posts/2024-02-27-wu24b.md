---
abstract: 'In Multi-label Learning (MLL), kernel methods and deep neural networks
  (DNNs) are two typical families of approaches. Recent theory discovers an interesting
  connection between infinitely wide DNNs and neural tangent kernel (NTK) based methods.
  Further, recent work has shown the promising performance of NTK-based methods in
  \emph{small-data single-labeled tasks}. Then, a natural question arises: can infinitely
  wide DNNs help small-data multi-label learning? To answer this question, in this
  paper, we present to utilize infinitely wide DNNs for the MLL task. Specifically,
  we propose an NTK-based kernel method for MLL, which aims to minimize Hamming and
  ranking loss simultaneously. Moreover, to efficiently train the model, we use the
  Nystr{รถ}m method, which has rarely been used in MLL. Further, we give rigorous theoretical
  analyses on learning guarantees of the proposed algorithm w.r.t. these two measures.
  Finally, empirical results on small-scale datasets illustrate its superior performance
  along with efficiency over several related baselines.'
section: Contributed Papers
title: Can Infinitely Wide Deep Nets Help Small-data Multi-label Learning?
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu24b
month: 0
tex_title: Can Infinitely Wide Deep Nets Help Small-data Multi-label Learning?
firstpage: 1494
lastpage: 1509
page: 1494-1509
order: 1494
cycles: false
bibtex_author: Wu, Guoqiang and Zhu, Jun
author:
- given: Guoqiang
  family: Wu
- given: Jun
  family: Zhu
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/wu24b/wu24b.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v222/wu24b/wu24b-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
