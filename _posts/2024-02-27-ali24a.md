---
abstract: Despite much research, Graph Neural Networks (GNNs) still do not display
  the favorable scaling properties of other deep neural networks such as Convolutional
  Neural Networks and Transformers. Previous work has identified issues such as oversmoothing
  of the latent representation and have suggested solutions such as skip connections
  and sophisticated normalization schemes. Here, we propose a different approach that
  is based on a stratification of the graph nodes. We provide motivation that the
  nodes in a graph can be stratified into those with a low degree and those with a
  high degree and that the two groups are likely to behave differently. Based on this
  motivation, we modify the Graph Neural Network (GNN) architecture so that the weight
  matrices are learned, separately, for the nodes in each group. This simple-to-implement
  modification seems to improve performance across datasets and GNN methods. To verify
  that this increase in performance is not only due to the added capacity, we also
  perform the same modification for random splits of the nodes, which does not lead
  to any improvement.
section: Contributed Papers
title: Degree-based stratification of nodes in Graph Neural Networks
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ali24a
month: 0
tex_title: Degree-based stratification of nodes in Graph Neural Networks
firstpage: 15
lastpage: 27
page: 15-27
order: 15
cycles: false
bibtex_author: Ali, Ameen and Wolf, Lior and Cevikalp, Hakan
author:
- given: Ameen
  family: Ali
- given: Lior
  family: Wolf
- given: Hakan
  family: Cevikalp
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/ali24a/ali24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
