---
abstract: Reducing the memory usage and computational complexity of high-performance
  deep neural networks while minimizing degradation of accuracy is a key issue in
  implementing these models on edge devices. To address this issue, partial quantization
  methods have been proposed to partially reduce the weight parameters of neural network
  models. However, the accuracy of existing methods degrades rapidly with increasing
  compression ratio. Although retraining can compensate for this issue to some extent,
  it is computationally very expensive. In this study, we propose a mixed-precision
  quantization algorithm without retraining or degradation in accuracy. In the proposed
  method, first, the difference between values after and before quantization losses
  of each channel in the layers of the pretrained model is calculated for all channels.
  Next, the layers are divided into two groups called semilayers according to whether
  the loss difference is positive or negative. The priorities for quantization in
  the semilayers are determined based on the Kulback-Leibler divergence derived from
  the probability distribution of the softmax output after and before quantization.
  The same process is repeated as a mixed-precision quantization while gradually decreasing
  the bitwidth, for example, with 8-, 6-, and 4-bit quantizations, and so forth. The
  results of an experimental evaluation show that the proposed method successfully
  compressed a ResNet-18 model by 81.44%, a ResNet-34 model by 84.25%, and a ResNet-50
  model by 80.39% on image classification tasks using the ImageNet dataset, and a
  ResNet-18 model by 80.56% on image classification tasks using the CIFAR-10 dataset,
  with no degradation of the inference accuracy of the pretrained models.
section: Contributed Papers
title: A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: matsumoto24a
month: 0
tex_title: A Mixed-Precision Quantization Method without Accuracy Degradation Using
  Semilayers
firstpage: 882
lastpage: 894
page: 882-894
order: 882
cycles: false
bibtex_author: Matsumoto, Kengo and Matsuda, Tomoya and Inoue, Atsuki and Kawaguchi,
  Hiroshi and Sakai, Yasufumi
author:
- given: Kengo
  family: Matsumoto
- given: Tomoya
  family: Matsuda
- given: Atsuki
  family: Inoue
- given: Hiroshi
  family: Kawaguchi
- given: Yasufumi
  family: Sakai
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/matsumoto24a/matsumoto24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
