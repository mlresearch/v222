---
abstract: We conduct a detailed investigation of tempered posteriors and uncover a
  number of crucial and previously undiscussed points. Contrary to previous results,
  we first show that for realistic models and datasets and the tightly controlled
  case of the Laplace approximation to the posterior, stochasticity does not in general
  improve test accuracy. The coldest temperature is often optimal. One might think
  that Bayesian models with some stochasticity can at least obtain improvements in
  terms of calibration. However, we show empirically that when gains are obtained
  this comes at the cost of degradation in test accuracy. We then discuss how targeting
  Frequentist metrics using Bayesian models provides a simple explanation of the need
  for a temperature parameter $\lambda$ in the optimization objective. Contrary to
  prior works, we finally show through a PAC-Bayesian analysis that the temperature
  $\lambda$ cannot be seen as simply fixing a misspecified prior or likelihood.
section: Contributed Papers
title: The Fine Print on Tempered Posteriors
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pitas24a
month: 0
tex_title: The Fine Print on Tempered Posteriors
firstpage: 1087
lastpage: 1102
page: 1087-1102
order: 1087
cycles: false
bibtex_author: Pitas, Konstantinos and Arbel, Julyan
author:
- given: Konstantinos
  family: Pitas
- given: Julyan
  family: Arbel
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/pitas24a/pitas24a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v222/pitas24a/pitas24a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
