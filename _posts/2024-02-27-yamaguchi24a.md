---
abstract: 'Semi-supervised learning (SSL) is a promising approach for training deep
  classification models using labeled and unlabeled datasets. However, existing SSL
  methods rely on a large unlabeled dataset, which may not always be available in
  many real-world applications due to legal constraints (e.g., GDPR). In this paper,
  we investigate the research question: \textit{Can we train SSL models without real
  unlabeled datasets?} Instead of using real unlabeled datasets, we propose an SSL
  method using synthetic datasets generated from generative foundation models trained
  on datasets containing millions of samples in diverse domains (e.g., ImageNet).
  Our main concepts are to identify synthetic samples that emulate unlabeled samples
  from generative foundation models and to train classifiers using these synthetic
  samples. To achieve this, our method is formulated as an alternating optimization
  problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers
  using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning
  objective that optimizes latent variables to generate samples that resemble real
  labeled samples and minimize the validation loss. For (ii), we propose a simple
  unsupervised loss function that regularizes the feature extractors of classifiers
  to maximize the performance improvement obtained from synthetic samples. We confirm
  that our method outperforms baselines using generative foundation models on SSL.
  We also demonstrate that our methods outperform SSL using real unlabeled datasets
  in scenarios with extremely small amounts of labeled datasets. This suggests that
  synthetic samples have the potential to provide improvement gains more efficiently
  than real unlabeled data.'
section: Contributed Papers
title: Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yamaguchi24a
month: 0
tex_title: Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples
firstpage: 1510
lastpage: 1525
page: 1510-1525
order: 1510
cycles: false
bibtex_author: Yamaguchi, Shin'ya
author:
- given: Shinâ€™ya
  family: Yamaguchi
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/yamaguchi24a/yamaguchi24a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v222/yamaguchi24a/yamaguchi24a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
