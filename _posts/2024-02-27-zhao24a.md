---
abstract: Large neural networks are often overparameterised and prone to overfitting,
  Dropout is a widely used regularization technique to combat overfitting and improve
  model generalization. However, unstructured Dropout is not always effective for
  specific network architectures and this has led to the formation of multiple structured
  Dropout approaches to improve model performance and, sometimes, reduce the computational
  resources required for inference. In this work, we revisit structured Dropout comparing
  different Dropout approaches on natural language processing and computer vision
  tasks for multiple state-of-the-art networks. Additionally, we devise an approach
  to structured Dropout we call \textbf{\emph{ProbDropBlock}} which drops contiguous
  blocks from feature maps with a probability given by the normalized feature salience
  values. We find that, with a simple scheduling strategy, the proposed approach to
  structured Dropout consistently improves model performance compared to baselines
  and other Dropout approaches on a diverse range of tasks and models. In particular,
  we show \textbf{\emph{ProbDropBlock}} improves RoBERTa finetuning on MNLI by $0.22%$,
  and training of ResNet50 on ImageNet by $0.28%$.
section: Contributed Papers
title: Revisiting Structured Dropout
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24a
month: 0
tex_title: Revisiting Structured Dropout
firstpage: 1699
lastpage: 1714
page: 1699-1714
order: 1699
cycles: false
bibtex_author: Zhao, Yiren and Dada, Oluwatomisin and Mullins, Robert and Gao, Xitong
author:
- given: Yiren
  family: Zhao
- given: Oluwatomisin
  family: Dada
- given: Robert
  family: Mullins
- given: Xitong
  family: Gao
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/zhao24a/zhao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
