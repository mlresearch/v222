---
abstract: As the most commonly used quantization techniques for deep neural networks,
  the int-only quantization methods use scale factor to linearly approximate the weights
  or activation of each layer. However, when passing activation data between layers,
  such int-only quantization methods require extra Scale Factor Conversion (SFC) operations,
  resulting in computational overhead. In this paper, we propose a Group-Wise Quantization
  framework, called GWQ, to reduce computational consumption during the activation
  data pass process by allowing multiple layers share one scale factor in SFC operations.
  Specifically, in the GWQ framework, we propose two algorithms for network layers
  grouping and model training. For the grouping of network layers, we propose a grouping
  algorithm based on the similarity of data numerical distribution. Then, the network
  layers divided into the same group will be quantified using the same common scale
  factor to reduce the computational consumption. Considering the additional performance
  loss caused by sharing scale factors among multiple layers, we propose a training
  algorithm to optimize these shared scale factors and model parameters, by designing
  a learnable power-of-two scaling parameter for each layer. Extensive experiments
  demonstrate that the proposed GWQ framework is able to effectively reduce the computational
  burden during inference, while maintaining model performance with negligible impact.
section: Contributed Papers
title: 'GWQ: Group-Wise Quantization Framework for Neural Networks'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang24a
month: 0
tex_title: "{GWQ}: {G}roup-Wise Quantization Framework for Neural Networks"
firstpage: 1526
lastpage: 1541
page: 1526-1541
order: 1526
cycles: false
bibtex_author: Yang, Jiaming and Tang, Chenwei and Yu, Caiyang and Lv, Jiancheng
author:
- given: Jiaming
  family: Yang
- given: Chenwei
  family: Tang
- given: Caiyang
  family: Yu
- given: Jiancheng
  family: Lv
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/yang24a/yang24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
