---
abstract: Medical visual question answering (Med-VQA) aims to correctly answer the
  medical question based on the given image. One of the major challenges is the scarcity
  of large professional labeled datasets for training, which poses obstacles to feature
  extraction, especially for medical images. To overcome it, we propose a method to
  learn transferable visual representation based on conditional denoising diffusion
  probabilistic model(conditional DDPM).Specifically, we collate a large amount of
  unlabeled radiological images and train a conditional DDPM with the paradigm of
  auto-encoder to obtain a model which can extract high-level semantic information
  from medical images.The pre-trained model can be used as a well initialized visual
  feature extractor and can be easily adapt to any Med-VQA systems. We build our Med-VQA
  system follow the state-of-the-art Med-VQA architecture and replace the visual extractor
  with our pre-trained model.Our proposal method outperforms the state-of-the-art
  Med-VQA method on VQA-RAD and achieves comparable result on SLAKE.
section: Contributed Papers
title: Diffusion-based Visual Representation Learning for Medical Question Answering
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bian24a
month: 0
tex_title: Diffusion-based Visual Representation Learning for Medical Question Answering
firstpage: 169
lastpage: 184
page: 169-184
order: 169
cycles: false
bibtex_author: Bian, Dexin and Wang, Xiaoru and Li, Meifang
author:
- given: Dexin
  family: Bian
- given: Xiaoru
  family: Wang
- given: Meifang
  family: Li
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/bian24a/bian24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
