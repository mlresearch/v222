---
abstract: Amazon ships billions of packages to its customers annually within the United
  States. Shipping cost of these packages are used on the day of shipping (day 0)
  to estimate profitability of sales. Downstream systems utilize these days 0 profitability
  estimates to make financial decisions, such as pricing strategies and delisting
  loss-making products. However, obtaining accurate shipping cost estimates on day
  0 is complex for reasons like delay in carrier invoicing or fixed cost components
  getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead
  to bad decision, such as pricing items too low or high, or promoting the wrong product
  to the customers. Current solutions for estimating shipping costs on day 0 rely
  on tree-based models that require extensive manual engineering efforts. In this
  study, we propose a novel architecture called the Rate Card Transformer (RCT) that
  uses self-attention to encode all package shipping information such as package attributes,
  carrier information and route plan. Unlike other transformer-based tabular models,
  RCT has the ability to encode a variable list of one-to-many relations of a shipment,
  allowing it to capture more information about a shipment. For example, RCT can encode
  properties of all products in a package. Our results demonstrate that cost predictions
  made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover,
  the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer,
  by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate
  card that can improve the performance of tree-based models.
section: Contributed Papers
title: 'Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate
  Card Transformer'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sreekar24a
month: 0
tex_title: 'Unveiling the Power of Self-Attention for Shipping Cost Prediction: {T}he
  Rate Card Transformer'
firstpage: 1263
lastpage: 1275
page: 1263-1275
order: 1263
cycles: false
bibtex_author: Sreekar, P Aditya and Verma, Sahil and Madhavan, Varun and Persad,
  Abhishek
author:
- given: P Aditya
  family: Sreekar
- given: Sahil
  family: Verma
- given: Varun
  family: Madhavan
- given: Abhishek
  family: Persad
date: 2024-02-27
address:
container-title: Proceedings of the 15th Asian Conference on Machine Learning
volume: '222'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 2
  - 27
pdf: https://proceedings.mlr.press/v222/sreekar24a/sreekar24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
